# -*- coding: utf-8 -*-
"""SENTIMENT ANALYSIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kmIIegNn1f33X8Z3ibiKgOpIaXY1JNs4

#SENTIMENT ANALYSIS

##IMPORTING THE LIBRARIES
"""

##IMPORTING THE LIBRARIES
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random                       #FOR PSEUDO RANDOM NUMBER GENERATION
from wordcloud import WordCloud     #TO VISUALIZE THE TEXT DATA
from wordcloud import STOPWORDS

"""##READING THE DATA"""

#MOUNTING THE DRIVE
from google.colab import drive
drive.mount('/content/drive')

#READING THE CSV FILE
DF = pd.read_csv('/content/drive/MyDrive/SEM 5/ML/Sentiment Analysis.csv')
print(DF)

"""##DATA PREPROCESSING

"""

#PRINTING THE COLUMN NAMES
print(DF.columns)

print(DF.info())

"""The last 5 columns are not needed in the dataframe.Their values are also Null.Hence we need to drop them.

It is also seen that there are no Null Values other columns.
"""

#DROPPING THE LAST FIVE COLUMNS
DF=DF.iloc[:,:-5]
print(DF)

#DESCRIBING THE DATA
print(DF.describe())

#PRINTING THE DATATYPES OF THE DATASET
print(DF.dtypes)

#PRINTING THE UNIQUE VALUES IN EACH COLUMN OF THE DATASET
print("UNIQUE VALUES IN THE COLUMN - drugName")
UV=pd.unique(DF['drugName'])
print(UV)
print(len(list(UV)))
print("UNIQUE VALUES IN THE COLUMN - condition")
UV=pd.unique(DF['condition'])
print(len(list(UV)))
print("UNIQUE VALUES IN THE COLUMN - Sentiment")
UV=pd.unique(DF['Sentiment'])
print(len(list(UV)))

"""We infer that there are 588 unique Drugs in the dataset,there are 40 medical conditions in the dataset."""

#CHECKING FOR DUPLICATES
DF.duplicated().value_counts()

"""It can be seen that there are 181 duplicates/redundant data.These are not needed and misleading as well by increasing the overall count.Hence we need to remove the duplicates."""

#REMOVING ALL THE DUPLICATES
DF = DF.drop_duplicates()
DF.duplicated().value_counts()

"""**PREPROCESSING USING NLTK(NATURAL LANGUAGE TOOL KIT)**"""

#IMPORTING THE NLTK PACKAGE
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer

#DOWLOADING THE NLTK RESOURCES
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('vader_lexicon')

"""**FINDING THE SENTIMENT VALUE OF ALL THE TEXTS USING NLTK**"""

#INITIALIZING THE SENTIMENT INTENSITY ANALYSER
sia = SentimentIntensityAnalyzer()

#FUNCTION TO APPLY SENTIMENT ANALYSIS FOR SINGLE TEXT
def analyze_sentiment(text):
    sentiment_scores = sia.polarity_scores(text)
    return sentiment_scores['compound']

#APPLYING THE SENTIMENT ANALYSIS SCORE FOR THE TEXT DATA
DF['sentiment_score'] = DF['review'].apply(analyze_sentiment)
DF

"""**Here** we have used the inbuilt SentimentIntensityAnalyzer resource to calculate the sentiment score.With this we now try to understand what 0 and 1 represent."""

def predict_class(score):
  if score < 0:
    return 0;
  else:
    return 1;

DF['Predicted_sentiment_score'] = DF['sentiment_score'].apply(predict_class)
#COUNTING THE NUMBER OF MISMATCHES
count=0
for index,row in DF.iterrows():
  if row['Predicted_sentiment_score']!=row['Sentiment']:
    count+=1
print(count*100/len(DF))

"""We can see that using this NLTK resource, 45% of the predicted value is wrong.Hence it is not a good way to predict.

**USING THE NLTK LIBRARY WE PERFORM THE FOLLOWING FUNCTIONS TO MAKE THE DATA FIT FOR MODELLING**

1.Tokenizes the text into words.

2.Removes common English stopwords.

3.Lemmatizes the remaining words.
"""

#DEFINING FUCNTIONS TO PERFORM THE NLTK PREPROCESSING STEPS

#1.USED TO TOKENIZE THE TEXT DATA                                               - EX : I AM ASH =>['I','AM','ASH']
def tokenize_text(text):
    tokens = word_tokenize(text)
    return tokens
#Tokenization is the process of splitting a text into individual words or units.

#2.USED TO REMOVE COMMON ENGLISH STOPWORDS FROM TOKENISED DATA
def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    return filtered_tokens
#Stopwords are words that are considered to be of little value in text analysis because they are very common and don't carry much meaningful information (e.g., "the," "and," "in").
#We use the NLTK library's list of English stopwords to identify and remove them from the list of tokens.

# 3.USED TO NORMALIZE THE TOKEN                                                 - EX : ["loving", "cats"]  => ["love", "cat"]
def lemmatize_text(tokens):
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return lemmatized_tokens
#Lemmatization is the process of reducing words to their base or root form. It aims to normalize words so that different inflections or forms of the same word are represented by a common base form.
#It uses the WordNetLemmatizer from the NLTK library to perform lemmatization.

#4.MAIN FUNCTION TO PERFORM ALL NLTK PREPROCESSING
def preprocess_text(text):
    tokens = tokenize_text(text)
    tokens = remove_stopwords(tokens)
    tokens = lemmatize_text(tokens)
    return ' '.join(tokens)  # Join the tokens back into a single string

#5.SIMILAR TO LEMATIZATION
def stem_text(tokens):
    stemmer = PorterStemmer()
    stemmed_tokens = [stemmer.stem(word) for word in tokens]
    return stemmed_tokens

#CALLING THE preprocess_text FUNCTION
DF['review'] = DF['review'].apply(preprocess_text)

#STORING THE PREPROCESSED DATA INTO A SEPERATE CSV FILE
DF.to_csv('Preprocessed_DataFrame.csv',index=False)
DF2=pd.read_csv('/content/Preprocessed_DataFrame.csv')
DF2

"""**DETERMINING THE BAG OF WORDS FOR THE DATASET**"""

from sklearn.feature_extraction.text import CountVectorizer

# Create a CountVectorizer instance
vectorizer = CountVectorizer()

#FIT AND TRANFORM THE PREPROCESSED DATA
bow_matrix = vectorizer.fit_transform(DF['review'])
#The result is a sparse matrix where each row corresponds to a document (review), and each column corresponds to a unique word in your dataset.

bow_array = bow_matrix.toarray()
# The vocabulary (unique words) can be accessed as follows:
vocabulary = vectorizer.get_feature_names_out()
bow_df = pd.DataFrame(bow_array, columns=vocabulary)
bow_df

"""The BoW representation converts text data into numerical format that machine learning algorithms can work with.

**CALCULATING THE TF-IDF FOR bow**

**Term Frequency (TF)**: This measures how frequently a term appears in a document. It is calculated as the number of times a term appears in a document divided by the total number of terms in that document.

**Inverse Document Frequency (IDF)**: This measures the importance of a term across a collection of documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.

The TF-IDF score for a term in a document is then calculated by multiplying the TF and IDF values for that term.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TfidfVectorizer instance
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the BoW DataFrame to obtain TF-IDF values
tfidf_matrix = tfidf_vectorizer.fit_transform(bow_df)

# Convert the tfidf_matrix to a DataFrame for better visualization (optional)
import pandas as pd
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Print the TF-IDF DataFrame
print(tfidf_df)

"""##VISUALIZATION

**PLOTTING THE PIE CHART TO REPRESENT THE PROPORTION SENTIMENTS IN THE WHOLE DATASET**
"""

#PLOTTING THE PIE CHART FOR THE COLUMN SENTIMENT
CLASS_COUNT=[]
CLASS_COUNT.append(DF['Sentiment'].value_counts()[1])
CLASS_COUNT.append(DF['Sentiment'].value_counts()[0])
print(CLASS_COUNT)

# Creating plot
fig,ax = plt.subplots()
Class=['1','0']
explode=[0,0.1]
ax.pie(CLASS_COUNT,labels=Class,explode=explode,autopct='%1.1f%%')
plt.legend(Class)
plt.title("THE SENTIMENT DATA CLASS")
plt.show()

"""**PLOTTING THE WORD CLOUD FOR DRUG NAMES**"""

#CONCATENATING ALL THE DRUG NAMES FROM THE COLUMN 'drugName' INTO A SINGLE STRING
text_data = ' '.join(DF['drugName'].astype(str))
#CREWATING WORD CLOUD OBJECT
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Hide the axis
plt.show()

"""**PLOTTING THE TOP 10 DRUG NAMES**"""

plt.figure(figsize=(12,6))
conditions = DF['drugName'].value_counts(ascending = False).head(10)

plt.bar(conditions.index,conditions.values)
plt.title('Top 10 Drug Names',fontsize = 20)
plt.xticks(rotation=90)
plt.ylabel('count')
plt.show()

"""**PLOTTING THE BOTTOM 10 DRUG NAMES**

"""

plt.figure(figsize=(12,6))
conditions = DF['drugName'].value_counts(ascending = False).tail(10)

plt.bar(conditions.index,conditions.values)
plt.title('Bottom- 0 Drug Names',fontsize = 20)
plt.xticks(rotation=90)
plt.ylabel('count')
plt.show()

"""**PLOTTING THE WORD CLOUD FOR ALL CONDITIONS**"""

#CONCATENATING ALL THE CONDITIONS FROM THE COLUMN 'condition' INTO A SINGLE STRING
text_data = ' '.join(DF['condition'].astype(str))
#CREWATING WORD CLOUD OBJECT
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Hide the axis
plt.show()

"""**PLOTTING THE TOP 5 CONDITIONS**"""

plt.figure(figsize=(12,6))
conditions = DF['condition'].value_counts(ascending = False).head(5)

plt.bar(conditions.index,conditions.values)
plt.title('Top 05 Conditions',fontsize = 20)
plt.xticks(rotation=90)
plt.ylabel('count')
plt.show()

"""**PLOTTING THE BOTTOM 5 CONDITIONS**"""

plt.figure(figsize=(12,6))
conditions = DF['condition'].value_counts(ascending = False).tail(5)

plt.bar(conditions.index,conditions.values)
plt.title('Bottom 05 Conditions',fontsize = 20)
plt.xticks(rotation=90)
plt.ylabel('count')
plt.show()

"""**NUMBER OF DRUGS PER CONDITION**"""

#lets check the number of drugs/condition
result = DF.groupby('condition')['drugName'].nunique().sort_values(ascending=False).head(20)

# Create a bar chart
plt.figure(figsize=(12, 6))
result.plot(kind='bar', color='skyblue')
plt.title('Top 20 Conditions with the Most Unique Drugs')
plt.xlabel('Condition')
plt.ylabel('Number of Unique Drugs')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability

# Display the plot
plt.tight_layout()
plt.show()

"""**PLOTTING THE TOP 20 NUMBER OF CONDITIONS PER DRUG**"""

#lets check the number of drugs present in our dataset condition wise
conditions_gp = DF.groupby('drugName')['condition'].nunique().sort_values(ascending=False)

#plot the top 20
# Setting the Parameter
condition_gp_top_20 = conditions_gp.head(20)
sns.set(font_scale = 1.2, style = 'darkgrid')
plt.rcParams['figure.figsize'] = [12, 6]
sns.barplot(x = condition_gp_top_20.index, y = condition_gp_top_20.values)
plt.title('Top-20 Number of condition per drugs',fontsize=20)
plt.xticks(rotation=90)
plt.ylabel('count',fontsize=10)
plt.show()

"""**WORD CLOUD OF POSITIVE REVIEWS**"""

text_data=" "
for index,row in DF.iterrows():
  if row['Sentiment']==1:
    text_data += str(row['review'])

#CREATING WORD CLOUD OBJECT
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Hide the axis
plt.show()

"""**WORD CLOUD OF NEGATIVE REVIEWS**"""

text_data=" "
for index,row in DF.iterrows():
  if row['Sentiment']==0:
    text_data += str(row['review'])

#CREATING WORD CLOUD OBJECT
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Hide the axis
plt.show()

"""##MODEL BUILDING AND EVALUATION

**KNN MODEL**
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

#Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(DF2['review'], DF2['Sentiment'], test_size=0.2, random_state=42)

#Create a TfidfVectorizer to convert text data to TF-IDF features
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

#Create and train a KNN classifier
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)
knn_classifier.fit(X_train_tfidf, y_train)

# Make predictions on the test data
y_pred = knn_classifier.predict(X_test_tfidf)

print(y_pred)

# Evaluate the model
print(classification_report(y_test, y_pred))

"""**NAIVE BAYES MODEL**"""

from sklearn.naive_bayes import MultinomialNB

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(DF2['review'], DF2['Sentiment'], test_size=0.2, random_state=42)

# Create a TfidfVectorizer to convert text data to TF-IDF features
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Create and train a Multinomial Naive Bayes classifier
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_tfidf, y_train)

# Make predictions on the test data
y_pred = nb_classifier.predict(X_test_tfidf)

# Evaluate the model
print(classification_report(y_test, y_pred))

"""**SVM MODEL - WITH DIFFERENT KERNEL FUNCTIONS**"""

from sklearn.svm import SVC

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(DF2['review'], DF2['Sentiment'], test_size=0.2, random_state=42)

# Create a TfidfVectorizer to convert text data to TF-IDF features
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

"""KERNEL FUNCTION - LINEAR"""

# Create and train SVM classifiers with different kernels
svm_linear = SVC(kernel='linear', C=1.0)
# Train the SVM classifiers
svm_linear.fit(X_train_tfidf, y_train)
# Make predictions on the test data for each model
y_pred_linear = svm_linear.predict(X_test_tfidf)
# Evaluate each model
print("Linear Kernel SVM:")
print(classification_report(y_test, y_pred_linear))

"""KERNEL FUNCTION - POLYNOMIAL"""

# Create and train SVM classifiers with different kernels
svm_poly = SVC(kernel='poly', degree=3, C=1.0)
# Train the SVM classifiers
svm_poly.fit(X_train_tfidf, y_train)
# Make predictions on the test data for each model
y_pred_poly = svm_poly.predict(X_test_tfidf)
# Evaluate each model
print("Polynomial Kernel SVM:")
print(classification_report(y_test, y_pred_poly))

"""KERNEL FUNCTION - RBF"""

# Create and train SVM classifiers with different kernels
svm_rbf = SVC(kernel='rbf', C=1.0)
# Train the SVM classifiers
svm_rbf.fit(X_train_tfidf, y_train)
# Make predictions on the test data for each model
y_pred_rbf = svm_rbf.predict(X_test_tfidf)
# Evaluate each model
print("RBF Kernel SVM:")
print(classification_report(y_test, y_pred_rbf))

"""**LOGISTIC REGRESSION**"""

from sklearn.linear_model import LogisticRegression

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(DF2['review'], DF2['Sentiment'], test_size=0.2, random_state=42)

# Create a TfidfVectorizer to convert text data to TF-IDF features
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Create and train a Logistic Regression classifier
logistic_regression = LogisticRegression(max_iter=1000)  # You can adjust hyperparameters as needed
logistic_regression.fit(X_train_tfidf, y_train)

# Make predictions on the test data
y_pred = logistic_regression.predict(X_test_tfidf)

# Evaluate the model
print(classification_report(y_test, y_pred))

"""We can see that out of all the models the SVM Model with polynomial kernel function has the highest accuracy

##EXPLAINABLE ML ALGORITHMS - LIME AND SHAP

LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) are powerful tools for interpreting machine learning models
"""

!pip install lime shap

import lime
import lime.lime_text
from lime.lime_text import LimeTextExplainer
import shap
from sklearn.pipeline import make_pipeline

"""###**KNN**

**APPLYING THE LIME ALGORITM FOR KNN MODEL**
"""

# Create a LIME Explainer
explainer = lime.lime_text.LimeTextExplainer(class_names=['class_label_0', 'class_label_1'])

# Select a random test instance (you can choose any instance)
test_instance = X_test.iloc[0]

# Create a pipeline with your KNN classifier and TF-IDF vectorizer
pipeline = make_pipeline(tfidf_vectorizer, knn_classifier)

# Explain the model's prediction for the selected instance
explanation = explainer.explain_instance(test_instance, pipeline.predict_proba)

# Visualize the explanation
explanation.show_in_notebook()

"""**APPLYING THE SHAP ALGORITM FOR KNN MODEL**

SHAP cannot be applied for KNN- Algorithm because it can be used only for callable models and KNN is an instance based algorithm.

###**NAIVE BAYES CLASSIFIER**

**APPLYING THE LIME ALGORITM FOR NAIVE BAYES MODEL**
"""

# Create a LIME Explainer
explainer = lime.lime_text.LimeTextExplainer(class_names=['class_label_0', 'class_label_1'])

# Select a random test instance (you can choose any instance)
test_instance = X_test.iloc[0]

# Create a pipeline with your Naive Bayes classifier and TF-IDF vectorizer
pipeline = make_pipeline(tfidf_vectorizer, nb_classifier)

# Explain the model's prediction for the selected instance
explanation = explainer.explain_instance(test_instance, pipeline.predict_proba)

# Visualize the explanation
explanation.show_in_notebook()

"""**APPLYING THE SHAP ALGORITM FOR NAIVE BAYES MODEL**

SHAP cannot be applied for Naive Bayes Classifier Model because it can be used only for callable models.

NOTE : THE ALTERNATE MODEL THAT CAN BE USED TO DETERMINE THE SHAP ALGORITHM'S INFERENCE FOR KNN AND NAIVE BAYES IS LOGISTIC REGRESSION

###**LOGISTIC REGRESSION**

**APPLYING THE LIME ALGORITM FOR LOGISTIC REGRESSION MODEL**
"""

# Create a LIME Explainer
explainer = lime.lime_text.LimeTextExplainer(class_names=['class_label_0', 'class_label_1'])

# Select a random test instance
test_instance = X_test.iloc[0]

# Create a pipeline with your logistic regression model and TF-IDF vectorizer
pipeline = make_pipeline(tfidf_vectorizer, logistic_regression)

# Explain the model's prediction for the selected instance
explanation = explainer.explain_instance(test_instance, pipeline.predict_proba)

# Visualize the explanation
explanation.show_in_notebook()

"""LIME generates a local, interpretable explanation for the model's prediction on a particular instance.

**APPLYING THE SHAP ALGORITM FOR LOGISTIC REGRESSION MODEL**
"""

# Create a SHAP explainer for the logistic regression model
explainer = shap.Explainer(logistic_regression, X_train_tfidf)

# Calculate SHAP values for the test dataset
shap_values = explainer.shap_values(X_test_tfidf)

# Summary plot for feature importance
shap.summary_plot(shap_values, X_test_tfidf, feature_names=tfidf_vectorizer.get_feature_names_out())

"""SHAP provides a global view of feature importance and contributions to each prediction.

##UI INTERFACE
"""

!pip install -q streamlit

!npm install localtunnel

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# ##IMPORTING THE LIBRARIES
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import random                       #FOR PSEUDO RANDOM NUMBER GENERATION
# from wordcloud import WordCloud     #TO VISUALIZE THE TEXT DATA
# from wordcloud import STOPWORDS
# 
# #IMPORTING THE NLTK PACKAGE
# import nltk
# from nltk.corpus import stopwords
# from nltk.tokenize import word_tokenize
# from nltk.stem import PorterStemmer
# from nltk.stem import WordNetLemmatizer
# from nltk.sentiment import SentimentIntensityAnalyzer
# 
# #DOWLOADING THE NLTK RESOURCES
# nltk.download('punkt')
# nltk.download('wordnet')
# nltk.download('stopwords')
# nltk.download('vader_lexicon')
# 
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report
# from sklearn.linear_model import LogisticRegression
# 
# # Import the Streamlit library
# import streamlit as st
# 
# 
# #DEFINING FUCNTIONS TO PERFORM THE NLTK PREPROCESSING STEPS
# 
# #1.USED TO TOKENIZE THE TEXT DATA                                               - EX : I AM ASH =>['I','AM','ASH']
# def tokenize_text(text):
#     tokens = word_tokenize(text)
#     return tokens
# #Tokenization is the process of splitting a text into individual words or units.
# 
# #2.USED TO REMOVE COMMON ENGLISH STOPWORDS FROM TOKENISED DATA
# def remove_stopwords(tokens):
#     stop_words = set(stopwords.words('english'))
#     filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
#     return filtered_tokens
# #Stopwords are words that are considered to be of little value in text analysis because they are very common and don't carry much meaningful information (e.g., "the," "and," "in").
# #We use the NLTK library's list of English stopwords to identify and remove them from the list of tokens.
# 
# # 3.USED TO NORMALIZE THE TOKEN                                                 - EX : ["loving", "cats"]  => ["love", "cat"]
# def lemmatize_text(tokens):
#     lemmatizer = WordNetLemmatizer()
#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]
#     return lemmatized_tokens
# #Lemmatization is the process of reducing words to their base or root form. It aims to normalize words so that different inflections or forms of the same word are represented by a common base form.
# #It uses the WordNetLemmatizer from the NLTK library to perform lemmatization.
# 
# #4.MAIN FUNCTION TO PERFORM ALL NLTK PREPROCESSING
# def preprocess_text(text):
#     tokens = tokenize_text(text)
#     tokens = remove_stopwords(tokens)
#     tokens = lemmatize_text(tokens)
#     return ' '.join(tokens)  # Join the tokens back into a single string
# 
# #5.SIMILAR TO LEMATIZATION
# def stem_text(tokens):
#     stemmer = PorterStemmer()
#     stemmed_tokens = [stemmer.stem(word) for word in tokens]
#     return stemmed_tokens
# 
# #READING THE CSV FILE
# DF2 = pd.read_csv('/content/Preprocessed_DataFrame.csv')
# print(DF2)
# 
# 
# #-------------------------------------------------
# #KNN
# #--------------------------------------------------
# #Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(DF2['review'], DF2['Sentiment'], test_size=0.2, random_state=42)
# 
# #Create a TfidfVectorizer to convert text data to TF-IDF features
# tfidf_vectorizer = TfidfVectorizer()
# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
# X_test_tfidf = tfidf_vectorizer.transform(X_test)
# 
# #Create and train a KNN classifier
# knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)
# knn_classifier.fit(X_train_tfidf, y_train)
# 
# # Make predictions on the test data
# #y_pred = knn_classifier.predict(X_test_tfidf)
# 
# #----------------------------------
# #LOGISTIC REGRESSION
# #----------------------------------
# # Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(DF2['review'], DF2['Sentiment'], test_size=0.2, random_state=42)
# # Create a TfidfVectorizer to convert text data to TF-IDF features
# tfidf_vectorizer = TfidfVectorizer()
# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
# X_test_tfidf = tfidf_vectorizer.transform(X_test)
# 
# # Create and train a Logistic Regression classifier
# logistic_regression = LogisticRegression(max_iter=1000)  # You can adjust hyperparameters as needed
# logistic_regression.fit(X_train_tfidf, y_train)
# 
# 
# # Set the heading
# st.markdown("# DRUG REVIEW SENTIMENT ANALYSIS")
# 
# # Set the subheading in a smaller font
# st.markdown("### ML LAB TEST - 2")
# 
# # Add a title for your app
# st.title("TEST DATA INPUT")
# 
# # Add a text input field where the user can enter the drug name
# drug_name = st.text_input("ENTER THE NAME OF THE DRUG : ", ' ')
# 
# # Add a text input field where the user can enter the condition name
# condition_name = st.text_input("ENTER THE CONDITION FOR WHICH THE DRUG IS USED : ", " ")
# 
# # Add a text input field where the user can enter the review
# review = str(st.text_input("ENTER THE REVIEW OF THE DRUG : ", " "))
# 
# # Create a dictionary to hold user inputs
# user_input = {'drugName': [drug_name], 'condition': [condition_name], 'review': [review]}
# 
# # Convert the dictionary to a DataFrame
# test_data = pd.DataFrame(user_input)
# 
# # Add a button to submit the user input
# if st.button("Submit"):
#     st.success(f"The Drug {drug_name} is used for the condition {condition_name}.\nREVIEW : {review}")
#     # Display the user input DataFrame
#     st.success("User Input DataFrame:")
#     st.write(test_data)
#     #PROCESSING THE TEST DATA
#     #CALLING THE preprocess_text FUNCTION
#     test_data['review'] = test_data['review'].apply(preprocess_text)
# 
#     # PREDICTION USING LOGARITHMIC REGRESSION ALGORITHM
#     st.markdown("PREDICTED SENTIMENT USING LOGISTIC REGRESSION ALGORITHM")
#     # Transform the user input using the same TF-IDF vectorizer
#     test_tfidf = tfidf_vectorizer.transform(test_data['review'])
#     # Make predictions on the test data
#     y_pred = logistic_regression.predict(test_tfidf)
#     y_pred = y_pred[0]
#     st.success(f"The predicted sentiment is: {y_pred}")
# 
#     st.markdown("PREDICTED SENTIMENT USING KNN ALGORITHM")
#     # Transform the user input using the same TF-IDF vectorizer
#     test_tfidf = tfidf_vectorizer.transform(test_data['review'])
#     # Make predictions on the test data
#     y_pred_knn = knn_classifier.predict(test_tfidf)
#     y_pred_knn = y_pred_knn[0]
#     st.success(f"The predicted sentiment using KNN is: {y_pred_knn}")
#

!streamlit run /content/app.py &>/content/logs.txt & curl ipv4.icanhazip.com

!npx localtunnel --port 8501